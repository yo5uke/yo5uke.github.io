---
title: "【スクレイピング】[rvest]{.fira-code}を使ってe-Statからファイルを取得する"
description: |
  Rのrvestパッケージを使って、e-Statの境界データを取得していきます！
date: 2025-03-19
date-modified: last-modified
categories:
  - R
  - データ処理
image: image/html_view.png
editor: visual
---

## はじめに

今回はRでスクレイピングを行っていきます。`rvest`パッケージを使ったスクレイピング自体は他のウェブサイトでも多数紹介されているのですが、今回JavaScriptが使われているページ（特にe-Stat）にも対応した方法をまとめます。これを行うには既に紹介されている方法からもう一工夫する必要があり、それも少々面倒です。

これまで僕はブラウザ拡張機能の「DownThemAll!」[^1]を使って無理くり実行していたのですが、この手の方法は再現が難しいし手間という欠点があるので、できればR上でコードとして残しておきたいと思っていました。そんなところ友人からあるページを紹介してもらい、抱えていた課題[^2]が解決できそうでしたので、これを機にまとめていきます。

[^1]: Chromeではサービスが終了したみたいです。Firefoxではまだあるようです。

[^2]: 先述のJavaScriptが使われているページで実行できないという課題

紹介してもらったページはこちらです↓

::: {.callout-tip appearance="minimal"}
[rvestで動的サイトをスクレイピングする（Seleniumを使わずに）](https://uchidamizuki.quarto.pub/blog/posts/2024/02/scraping-dynamic-sites-with-rvest.html)
:::

また、Google Chromeを使用するので、インストールしていない方はしておいてください。

## 使用するパッケージ

使用するパッケージは以下の通りです。Rプロジェクトを使うなり`setwd()`を使うなりでワーキングディレクトリを現在のディレクトリに設定しておいてください。プロジェクトについては[こちら](/pages/tips/2024/05/240515_rproj/index.html)。

ついでに、保存するディレクトリ（`data`フォルダ内の`shpfiles`）をあらかじめ指定しておきます。なければ作成する関数もつけておきます。

``` r
library(rvest)
library(here)

save_dir <- here("data/shpfiles")
# ダウンロード用のフォルダがなければ作成する
if (!dir.exists(save_dir)) {
  dir.create(save_dir)
}
```

## ファイルをダウンロードするページ

今回ファイルを取得していくのは、e-Statの「地図」→「境界データダウンロード」→「3次メッシュ」→「世界測地系平面直角座標系・Shapefile」のページです。

<https://www.e-stat.go.jp/gis/statmap-search?page=1&type=2&aggregateUnitForBoundary=S&coordsys=2&format=shape>

メッシュいうのは国土を例えば1km×1kmの正方形で区切ったものをいい、その中の人口等のデータを扱うことができるようになります。このあたりの詳細は以下の書籍が非常にわかりやすいのでおすすめです。RでGISを扱う方法も学べます。

::: {.callout-tip appearance="minimal"}
[事例で学ぶ経済・政策分析のためのGIS入門: QGIS,R,GeoDa対応](https://www.amazon.co.jp/%E4%BA%8B%E4%BE%8B%E3%81%A7%E5%AD%A6%E3%81%B6%E7%B5%8C%E6%B8%88%E3%83%BB%E6%94%BF%E7%AD%96%E5%88%86%E6%9E%90%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AEGIS%E5%85%A5%E9%96%80-QGIS-GeoDa%E5%AF%BE%E5%BF%9C-%E6%B2%B3%E7%AB%AF-%E7%91%9E%E8%B2%B4/dp/4772242309)
:::

## ページを取得

基本的には`read_html()`関数で読み込むことができるのですが、JavaScriptを使用しているような動的なページには対応していません。そこで使用するのが`read_html_live()`関数です。これらの関数の仕組み自体は冒頭で紹介したページがわかりやすいのでそちらを読んでみてください。

先ほどのURLを読み込んでみましょう。

``` r
html <- read_html_live("https://www.e-stat.go.jp/gis/statmap-search?page=1&type=2&aggregateUnitForBoundary=S&coordsys=2&format=shape")
```

## ファイルのリンクを取得

次にいよいよファイルを取得していきます。僕もHTMLに詳しいわけではないので詳細は書きませんが、`a`タグというのがリンクを生成するためのもので、その中に`href`という属性があります。例えば

``` html
<a class="stat-dl_icon stat-statistics-table_icon" tabindex="40" href="/gis/statmap-search/data?dlserveyId=S&amp;code=3036&amp;coordSys=2&amp;format=shape&amp;downloadType=5">
<span class="stat-dl_text">世界測地系平面直角座標系・Shapefile</span>
</a>
```

というようになっており、`a`と`/a`で囲まれている間に`href`やその他の属性が含まれていることがわかります。

::: {.callout-tip collapse="true"}
## どうやって探す？

このHTMLはどうやって探せばよいのでしょうか。実はブラウザ上から簡単に見ることができます。

ブラウザ上で任意のページを開いたら、`Ctrl` + `Shift` + `I`を同時に入力します。もしくはブラウザ右上の点々から「その他のツール」→「デベロッパー ツール」でも開けます。

![](image/developer_tool.png){fig-align="center" width="80%"}

すると画面右側になにやらぶわぁーっと出てきますね。これの上半分がHTMLコードです。ここから特定のコードを探すのは骨が折れそうですが、逆にページの要素をクリックすることで該当するコードを探すことができます。デベロッパーツール画面上部の左側にカーソルのようなアイコンがあります。

![一番左側です](image/dev_tool_bar.png){fig-align="center" width="80%"}

これをクリックしたうえでページ上の要素にカーソルを重ねてクリックすると、該当箇所のコードが表示されます。カーソルを「世界測地系平面直角座標系・Shapefile」という文字に重ねると先ほどのコードが表示されます。
:::

ここでは`a`タグを探す→`href`を探し取得する、という手順を踏みます。

``` r
links <- html |> 
  html_elements("a") |> 
  html_attr("href")
```

`html_elements("a")`で`a`タグを探し、`html_attr("href")`で属性（attribute）を探しています。`links`には`a`タグの中の`href`に入っているリンクがずらーっと入っていることになります。

ここでは表示しませんが、`links`の中身を確認すると、想像以上にたくさんのリンクが入っていることがわかります。ここからさらにシェープファイルのダウンロードリンクを探さなければなりません。

上の`a`タグの例を見てもらうと、`/gis/statmap-search/data?dlserveyId=S&code=3036&coordSys=2&format=shape&downloadType=5`というのがシェープファイルのダウンロードリンクであることがわかります。他のシェープファイルのリンクと見比べてもらうと、`code=3036`の部分だけがそれぞれ異なっており、他は同じです。すなわち、まとめてダウンロードしたければこの部分さえうまいことやれば可能になるということです。

とりあえず、数多のリンクの中からシェープファイルのリンクだけ抽出しておきましょう。`links`の中のリンクを確認すると、`data?dlserveyId=S&`という部分がシェープファイルのダウンロードリンクにのみ含まれていることがわかる、これを含むリンクを抽出すればよいことになります。

``` r
shp_links <- links[grepl("data\\?dlserveyId=S&", links)]
```

**ポイント：**

-   `grepl()`で条件に合致するものを抽出

    -   `grepl(x, y)`で、`y`の中で`x`を含むものに`TRUE`を返す

    -   `links`の中で`data\\?dlserveyId=S&`を含むものに`TRUE`を返している

    -   `?`の前に`\\`が入っているのは、正規表現の中で`?`は特別な役割を持っているので、その役割として認識されるのを防ぐため（エスケープしている）

-   `links[ ]` で、`TRUE`を持つものを抽出している

    -   すなわち`grepl()`で`TRUE`を返したもの（=シェープファイルのダウンロードリンクをもつもの）だけが残る

短いコードでもややこしいですね。特に記号のところには気を付けていただければと思います。

## リンクを整形する

お気づきの方もいらっしゃると思いますが、実は先ほどのリンクは相対的なものです。`https://`から始まっていません。ベースとして`https://www.e-stat.go.jp`というURLがあり、そこに続く形で`/gis/...`が入ってきます。

以下では取得したリンクにベースリンクをくっつけていきます。

``` r
base_url <- "https://www.e-stat.go.jp"
full_urls <- paste0(base_url, shp_links)
```

これでページ内の20個のリンクを抽出することができました。次にこれらのリンクを使ってダウンロードしていきます。

## ファイルをダウンロード

いよいよダウンロードをしていきます。

全てのリンクに対しダウンロードするコードを適用します。`for`の出番です。

``` r
for (i in seq_along(full_urls)) {
  # リンクからメッシュコードを取得
  code <- sub(".*code=([0-9]+).*", "\\1", shp_links[i])
  
  # ダウンロードするファイルのパスを指定する
  # 例: shpfiles/shpfile_3036.zip
  file_path <- file.path(save_dir, paste0("shpfile_", code, ".zip"))

  # ファイルをダウンロード
  download.file(full_urls[i], destfile = file_path, mode = "wb")
  
  # アクセス間隔を空ける
  # サーバー負荷を考慮
  Sys.sleep(2)
}
```

**ポイント：**

-   `seq_along(full_urls)`は`full_urls`の要素の数を取得している

    -   今回はURLが20個と分かっているが、数が多くて数えにくいような場合にも有用

-   `sub(".*code=([0-9]+).*", "\1", shp_links[i])`でメッシュコードを取得

    -   `sub(pattern, replacement, x)`で`x`の`pettern`を`replacement`に置換する

    -   `.*code=([0-9]+).*`はざっくり言うと、前後が何であれ`code=(何らかの数字)`というものを探してきて、括弧内の数字を取得しているということ

    -   `\\1`は今取得した数字を返すということ

    -   `x`には`shp_links[i]`が該当するので、`i`番目のリンクで上で述べた置換をしているということ

-   `file_path <- file.path(save_dir, paste0("shpfile_", code, ".zip"))`で保存する際のファイル名を作成

    -   現在のワーキングディレクトリにある`data`内の`shpfiles`というフォルダの中に`shpfile_3036.zip`というようなZipファイルを作成する

    -   ダウンロードはZipファイルで行われるので拡張子は`.zip`としておく

```         
my_project/
├── data/
├── shpfiles/  ← ここにダウンロードする
└── scripts/
    ├── download_script.R
    ├── analysis.R
    └── ...
```

-   `download.file(full_urls[i], destfile = file_path, mode = "wb")`でダウンロード

    -   `download.file(url, destfile, mode)`で`url`を`destfile`に`mode`で指定したモードでダウンロードする

    -   詳細は省くが、Zipファイルに対しては`mode="wb`を指定する

-   `Sys.sleep(2)`でサーバー負荷を軽減する

    -   作業ごとに2秒のインターバルを設けるということ

さすがに面倒すぎますね。最初は困ると思いますが、2回目以降大変便利に思えるはずです。

ひとまずこれでページ上のファイルをダウンロードすることができました！

## 全ページまとめてダウンロード

ここまでは1ページ内のファイルをダウンロードする方法について順を追って見てきました。

このセクションではシェープファイルが1～9ページにまたがって並んでいることを踏まえ、さらに`for`ループを構築し、これまでの作業を9ページにわたって実行できるようにします。

``` r
for (i in 1:9) {
  url <- paste0("https://www.e-stat.go.jp/gis/statmap-search?page=", i, "&type=2&aggregateUnitForBoundary=S&coordsys=1&format=shape")
  
  html <- read_html_live(url)
  Sys.sleep(1)
  
  links <- html |> 
    html_elements("a") |> 
    html_attr("href")
  
  shp_links <- links[grepl("data\\?dlserveyId=S&", links)]
  
  full_urls <- paste0(base_url, shp_links)
  
  for (j in seq_along(full_urls)) {
    code <- sub(".*code=([0-9]+).*", "\\1", shp_links[j])
    file_path <- file.path(save_dir, paste0("shpfile_", code, ".zip"))
    
    if (!file.exists(file_path)) {
      tryCatch({
        download.file(full_urls[j], destfile = file_path, mode = "wb")
      }, error = function(e) {})
      Sys.sleep(2)
    }
  }
}
```

**ポイント：**

1.  URLの`page=`の部分に1から9が入るように設定
    -   URLを分割し、ページ数の部分に`i`で数字を入れています。
2.  HTMLを読み込んだ後に`Sys.sleep(1)`で1秒待機
    -   HTMLを読み込んですぐ次のコマンドに移ると、中身を最後まで読み込まないまま次を実行してしまうので、しっかり読み込めるようにするための設定です。
3.  `for`ループ2段階目
    -   大外の`for`で`i`を使っているので、今度は`j`にしています。
    -   `if`でファイルがない場合に実行するようにしている。
        -   何らかの理由で途中エラーが発生し、再度全体を実行しても、既にダウンロードできているファイルを重複してダウンロードせずに済みます。
    -   `tryCatch`は、途中でエラーが起きたときでもスクリプト全体を止めずに柔軟に処理を続けるための関数で、エラーが起きても一旦スキップして最後まで実行します。

見慣れない関数も登場しましたが、このコードを実行すれば9ページ分まとめてダウンロードすることが可能です。

## コードまとめ

最後に今回のコードをまとめます。

``` r
library(rvest)
library(here)

for (i in 1:9) {
  url <- paste0("https://www.e-stat.go.jp/gis/statmap-search?page=", i, "&type=2&aggregateUnitForBoundary=S&coordsys=1&format=shape")
  
  html <- read_html_live(url)
  Sys.sleep(1)
  
  links <- html |> 
    html_elements("a") |> 
    html_attr("href")
  
  shp_links <- links[grepl("data\\?dlserveyId=S&", links)]
  
  full_urls <- paste0(base_url, shp_links)
  
  for (j in seq_along(full_urls)) {
    code <- sub(".*code=([0-9]+).*", "\\1", shp_links[j])
    file_path <- here("shpfiles", paste0("shpfile_", code, ".zip"))
    
    if (!file.exists(file_path)) {
      tryCatch({
        download.file(full_urls[j], destfile = file_path, mode = "wb")
      }, error = function(e) {})
      Sys.sleep(2)
    }
  }
}
```

## おわりに

今回はRでe-Statのシェープファイルを取得してきました。個人的にもずっとやりたいことだったので、不完全とはいえ、念願かなった感じです。

今後も調査して全ページを一つのループでダウンロードできるようなコードを書いていきたいと思います。

## おまけ

Zipファイルでダウンロードすることになりますが、今回の流れでエクスプローラーから直接展開したりファイルを削除するのは面倒だと思います。

解凍し、使用済みのZipファイルを削除するには以下を実行します。

``` r
zip_files <- list.files(save_dir, pattern = "\\.zip$", full.names = TRUE)

for (zip_path in zip_files) {
  unzip(zip_path, exdir = save_dir)
  file.remove(zip_path)
}
```

`unzip()`でZipファイルを解凍して、`file.remove()`でZipファイルを削除します。

## その他の実用例

最近仕事の一環で時間がかかりそうな作業をスクレイピングでサクッと解決したので、地味な例ではありますがご参考までに紹介します。

### シーン

各省庁が出している白書のリストが載っている[ページ](https://www.e-gov.go.jp/about-government/white-papers.html)があるのですが、ここから白書名、省庁名、リンクをぱぱーっと取ってきたいという場面です。

![こんな感じでずらーっと並んでいます](image/e-gov_list.png){fig-align="center" width="60%"}

### 手順

1.  `read_html()`でURLを読み込む（今回は`live`の方でなくてもできる例です）

```{r}
library(rvest)
library(tidyverse)

url <- "https://www.e-gov.go.jp/about-government/white-papers.html"
html <- read_html(url)
```

2.  要素を確認する
    -   `Ctrl` + `Shift` + `I`でデベロッパーツールを開き、白書名やリンクなどがどうなっているか確認します

![水循環白書はこのようになっています](image/developer_tool_hakusho.png){#hakusho-html fig-align="center"}

3.  要素を取得する

``` r
items <- html |> 
  html_element("main") |> 
  html_element("ul")

hakusho <- items |> 
  html_elements("span") |> 
  html_text()

ministry <- items |> 
  html_elements("p") |> 
  html_text()

links <- items |> 
  html_elements("a") |> 
  html_attr("href")
```

4.  データフレームにまとめる

``` r
df <- tibble::tibble(
  白書名 = hakusho, 
  省庁名 = ministry, 
  リンク = links
)
```

### 詳細

ブラウザ上でHTMLコードを下の方まで見てみるとわかるのですが、白書の情報は`<main role="main">`から`</main>`の間に含まれていることがわかります。さらに、[画像](#hakusho-html)をジーっと見てみると、欲しい要素は

1.  `span`で囲まれている部分
2.  `p`で囲まれている部分
3.  `a`タグの中の`href`

であることがわかります。試しにこれまで説明したコードを用いて取得してみると、

```{r}
hakusho <- html |> 
  html_elements("span")
```

```{r}
#| echo: false

print(hakusho)

```

欲しいのは「水循環白書」みたいな白書名だけですが、`<span>`の文字など余計な要素も取れてしまいました。中身のテキストだけ抜きたい場合は`html_text()`を使います。

```{r}
hakusho <- html |> 
  html_elements("span") |> 
  html_text()
```

```{r}
#| echo: false

print(head(hakusho, n = 10))

```

表示しているのは一部ですが、それでも、そもそも余計なものが多いことがわかります。これは、欲しいのが`<main>`で囲まれている部分であるにもかかわらず、ページ全体から`span`を探して拾ってきているためです。

さらに[画像](#hakusho-html)をよーく見ると、`main`の中でも`p`は「各行政機関が～」という文言も含んでしまっていることがわかります。`main`→`p`と指定すると、この文言も拾ってきてしまいます。そのため絞り込みとしては`main`、さらには`ul`にも絞った方が良いということが考えられます[^3]。すなわち`main`→`ul`→`p`という絞り方です。

[^3]: 絞り込み方はこれ以外にもいろいろあると思います。

まずは対象を`main`→`ul`に絞っておきます。

```{r}
items <- html |> 
  html_element("main") |> 
  html_element("ul")
```

`html_elements()`もありますが、これは複数形なので要素が多くあり、全部取りたいような場合に使います。今回は`main`が1つでその中の`ul`も1つとわかっているので単数形です。

この上で`items`を用いて取得します。

```{r}
hakusho <- items |> 
  html_elements("span") |> 
  html_text()
```

```{r}
#| echo: false

print(head(hakusho, n = 10))

```

これも表示しているのは一部ですが、しっかりとれています。同様の手順で要素を確認しつつ取得を進めます。

```{r}
ministry <- items |> 
  html_elements("p") |> 
  html_text()

links <- items |> 
  html_elements("a") |> 
  html_attr("href")
```

これで全53件分そろいました。最後にデータフレーム化して終了です。

```{r}
df <- tibble(
  白書名 = hakusho, 
  省庁名 = ministry, 
  リンク = links
)
```

```{r}
#| echo: false

tinytable::tt(head(df))

```

いい感じに整理できました。

ウェブサイトからたくさんの情報を繰り返し同じ動作で集めなければならないとき、役に立ちそうですね！

おまけもおまけで、もはや本題には関係ありませんが、厳密に白書と明記されているもののみをとりたいならデータフレームにフィルタリングしてしまいましょう。

```{r}
df_hakusho <- df |> 
  filter(str_detect(hakusho, "白書"))
```

```{r}
#| echo: false

tinytable::tt(head(df_hakusho))

```

「年次報告書」などは抜くことができています。`str_detect(変数名, パターン)`で対応するものを抜き取ることができます。
