{
  "hash": "8e701144fb288c3cd6e9822124c3b160",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"【スクレイピング】[rvest]{.fira-code}を使ってe-Statからファイルを取得する\"\ndescription: |\n  Rのrvestパッケージを使って、e-Statの境界データを取得していきます！\ndate: 2025-03-19\ncategories:\n  - R\n  - データ処理\neditor: visual\n---\n\n## はじめに\n\n今回はRでスクレイピングを行っていきます。`rvest`パッケージを使ったスクレイピング自体は他のウェブサイトでも多数紹介されているのですが、今回JavaScriptが使われているページ（特にe-Stat）にも対応した方法をまとめます。これを行うには既に紹介されている方法からもう一工夫する必要があり、それも少々面倒です。\n\nこれまで僕はブラウザ拡張機能の「DownThemAll!」[^1]を使って無理くり実行していたのですが、この手の方法は再現が難しいし手間という欠点があるので、できればR上でコードとして残しておきたいと思っていました。そんなところ友人からあるページを紹介してもらい、抱えていた課題[^2]が解決できそうでしたので、これを機にまとめていきます。\n\n[^1]: Chromeではサービスが終了したみたいです。Firefoxではまだあるようです。\n\n[^2]: 先述のJavaScriptが使われているページで実行できないという課題\n\n紹介してもらったページはこちらです↓\n\n::: {.callout-tip appearance=\"minimal\"}\n[rvestで動的サイトをスクレイピングする（Seleniumを使わずに）](https://uchidamizuki.quarto.pub/blog/posts/2024/02/scraping-dynamic-sites-with-rvest.html)\n:::\n\nまた、Google Chromeを使用するので、インストールしていない方はしておいてください。\n\n## 使用するパッケージ\n\n使用するパッケージは以下の通りです。Rプロジェクトを使うなり`setwd()`を使うなりでワーキングディレクトリを現在のディレクトリに設定しておいてください。プロジェクトについては[こちら](../240515_rproj/index.html)。\n\nついでに、保存するディレクトリ（`data`フォルダ内の`shpfiles`）をあらかじめ指定しておきます。なければ作成する関数もつけておきます。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\nlibrary(here)\n\nsave_dir <- here(\"data/shpfiles\")\n# ダウンロード用のフォルダがなければ作成する\nif (!dir.exists(save_dir)) {\n  dir.create(save_dir)\n}\n```\n:::\n\n\n## ファイルをダウンロードするページ\n\n今回ファイルを取得していくのは、e-Statの「地図」→「境界データダウンロード」→「3次メッシュ」→「世界測地系平面直角座標系・Shapefile」のページです。\n\n<https://www.e-stat.go.jp/gis/statmap-search?page=1&type=2&aggregateUnitForBoundary=S&coordsys=2&format=shape>\n\nメッシュいうのは国土を例えば1km×1kmの正方形で区切ったものをいい、その中の人口等のデータを扱うことができるようになります。このあたりの詳細は以下の書籍が非常にわかりやすいのでおすすめです。RでGISを扱う方法も学べます。\n\n::: {.callout-tip appearance=\"minimal\"}\n[事例で学ぶ経済・政策分析のためのGIS入門: QGIS,R,GeoDa対応](https://www.amazon.co.jp/%E4%BA%8B%E4%BE%8B%E3%81%A7%E5%AD%A6%E3%81%B6%E7%B5%8C%E6%B8%88%E3%83%BB%E6%94%BF%E7%AD%96%E5%88%86%E6%9E%90%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AEGIS%E5%85%A5%E9%96%80-QGIS-GeoDa%E5%AF%BE%E5%BF%9C-%E6%B2%B3%E7%AB%AF-%E7%91%9E%E8%B2%B4/dp/4772242309)\n:::\n\n## ページを取得\n\n基本的には`read_html()`関数で読み込むことができるのですが、JavaScriptを使用しているような動的なページには対応していません。そこで使用するのが`read_html_live()`関数です。これらの関数の仕組み自体は冒頭で紹介したページがわかりやすいのでそちらを読んでみてください。\n\n先ほどのURLを読み込んでみましょう。\n\n``` r\nhtml <- read_html_live(\"https://www.e-stat.go.jp/gis/statmap-search?page=1&type=2&aggregateUnitForBoundary=S&coordsys=2&format=shape\")\n```\n\n## ファイルのリンクを取得\n\n次にいよいよファイルを取得していきます。僕もHTMLに詳しいわけではないので詳細は書きませんが、`a`タグというのがリンクを生成するためのもので、その中に`href`という属性があります。例えば\n\n``` html\n<a class=\"stat-dl_icon stat-statistics-table_icon\" tabindex=\"40\" href=\"/gis/statmap-search/data?dlserveyId=S&amp;code=3036&amp;coordSys=2&amp;format=shape&amp;downloadType=5\">\n<span class=\"stat-dl_text\">世界測地系平面直角座標系・Shapefile</span>\n</a>\n```\n\nというようになっており、`a`と`/a`で囲まれている間に`href`やその他の属性が含まれていることがわかります。\n\n::: {.callout-tip collapse=\"true\"}\n## どうやって探す？\n\nこのHTMLはどうやって探せばよいのでしょうか。実はブラウザ上から簡単に見ることができます。\n\nブラウザ上で任意のページを開いたら、`Ctrl` + `Shift` + `I`を同時に入力します。もしくはブラウザ右上の点々から「その他のツール」→「デベロッパー ツール」でも開けます。\n\n![](image/developer_tool.png){fig-align=\"center\" width=\"80%\"}\n\nすると画面右側になにやらぶわぁーっと出てきますね。これの上半分がHTMLコードです。ここから特定のコードを探すのは骨が折れそうですが、逆にページの要素をクリックすることで該当するコードを探すことができます。デベロッパーツール画面上部の左側にカーソルのようなアイコンがあります。\n\n![一番左側です](image/dev_tool_bar.png){fig-align=\"center\" width=\"80%\"}\n\nこれをクリックしたうえでページ上の要素にカーソルを重ねてクリックすると、該当箇所のコードが表示されます。カーソルを「世界測地系平面直角座標系・Shapefile」という文字に重ねると先ほどのコードが表示されます。\n:::\n\nここでは`a`タグを探す→`href`を探し取得する、という手順を踏みます。\n\n``` r\nlinks <- html |> \n  html_elements(\"a\") |> \n  html_attr(\"href\")\n```\n\n`html_elements(\"a\")`で`a`タグを探し、`html_attr(\"href\")`で属性（attribute）を探しています。`links`には`a`タグの中の`href`に入っているリンクがずらーっと入っていることになります。\n\nここでは表示しませんが、`links`の中身を確認すると、想像以上にたくさんのリンクが入っていることがわかります。ここからさらにシェープファイルのダウンロードリンクを探さなければなりません。\n\n上の`a`タグの例を見てもらうと、`/gis/statmap-search/data?dlserveyId=S&code=3036&coordSys=2&format=shape&downloadType=5`というのがシェープファイルのダウンロードリンクであることがわかります。他のシェープファイルのリンクと見比べてもらうと、`code=3036`の部分だけがそれぞれ異なっており、他は同じです。すなわち、まとめてダウンロードしたければこの部分さえうまいことやれば可能になるということです。\n\nとりあえず、数多のリンクの中からシェープファイルのリンクだけ抽出しておきましょう。`links`の中のリンクを確認すると、`data?dlserveyId=S&`という部分がシェープファイルのダウンロードリンクにのみ含まれていることがわかる、これを含むリンクを抽出すればよいことになります。\n\n``` r\nshp_links <- links[grepl(\"data\\\\?dlserveyId=S&\", links)]\n```\n\n**ポイント：**\n\n-   `grepl()`で条件に合致するものを抽出\n\n    -   `grepl(x, y)`で、`y`の中で`x`を含むものに`TRUE`を返す\n\n    -   `links`の中で`data\\\\?dlserveyId=S&`を含むものに`TRUE`を返している\n\n    -   `?`の前に`\\\\`が入っているのは、正規表現の中で`?`は特別な役割を持っているので、その役割として認識されるのを防ぐため（エスケープしている）\n\n-   `links[ ]` で、`TRUE`を持つものを抽出している\n\n    -   すなわち`grepl()`で`TRUE`を返したもの（=シェープファイルのダウンロードリンクをもつもの）だけが残る\n\n短いコードでもややこしいですね。特に記号のところには気を付けていただければと思います。\n\n## リンクを整形する\n\nお気づきの方もいらっしゃると思いますが、実は先ほどのリンクは相対的なものです。`https://`から始まっていません。ベースとして`https://www.e-stat.go.jp`というURLがあり、そこに続く形で`/gis/...`が入ってきます。\n\n以下では取得したリンクにベースリンクをくっつけていきます。\n\n``` r\nbase_url <- \"https://www.e-stat.go.jp\"\nfull_urls <- paste0(base_url, shp_links)\n```\n\nこれでページ内の20個のリンクを抽出することができました。次にこれらのリンクを使ってダウンロードしていきます。\n\n## ファイルをダウンロード\n\nいよいよダウンロードをしていきます。\n\n全てのリンクに対しダウンロードするコードを適用します。`for`の出番です。\n\n``` r\nfor (i in seq_along(full_urls)) {\n  # リンクからメッシュコードを取得\n  code <- sub(\".*code=([0-9]+).*\", \"\\\\1\", shp_links[i])\n  \n  # ダウンロードするファイルのパスを指定する\n  # 例: shpfiles/shpfile_3036.zip\n  file_path <- file.path(save_dir, paste0(\"shpfile_\", code, \".zip\"))\n\n  # ファイルをダウンロード\n  download.file(full_urls[i], destfile = file_path, mode = \"wb\")\n  \n  # アクセス間隔を空ける\n  # サーバー負荷を考慮\n  Sys.sleep(2)\n}\n```\n\n**ポイント：**\n\n-   `seq_along(full_urls)`は`full_urls`の要素の数を取得している\n\n    -   今回はURLが20個と分かっているが、数が多くて数えにくいような場合にも有用\n\n-   `sub(\".*code=([0-9]+).*\", \"\\1\", shp_links[i])`でメッシュコードを取得\n\n    -   `sub(pattern, replacement, x)`で`x`の`pettern`を`replacement`に置換する\n\n    -   `.*code=([0-9]+).*`はざっくり言うと、前後が何であれ`code=(何らかの数字)`というものを探してきて、括弧内の数字を取得しているということ\n\n    -   `\\\\1`は今取得した数字を返すということ\n\n    -   `x`には`shp_links[i]`が該当するので、`i`番目のリンクで上で述べた置換をしているということ\n\n-   `file_path <- file.path(save_dir, paste0(\"shpfile_\", code, \".zip\"))`で保存する際のファイル名を作成\n\n    -   現在のワーキングディレクトリにある`data`内の`shpfiles`というフォルダの中に`shpfile_3036.zip`というようなZipファイルを作成する\n\n    -   ダウンロードはZipファイルで行われるので拡張子は`.zip`としておく\n\n```         \nmy_project/\n├── data/\n├── shpfiles/  ← ここにダウンロードする\n└── scripts/\n    ├── download_script.R\n    ├── analysis.R\n    └── ...\n```\n\n-   `download.file(full_urls[i], destfile = file_path, mode = \"wb\")`でダウンロード\n\n    -   `download.file(url, destfile, mode)`で`url`を`destfile`に`mode`で指定したモードでダウンロードする\n\n    -   詳細は省くが、Zipファイルに対しては`mode=\"wb`を指定する\n\n-   `Sys.sleep(2)`でサーバー負荷を軽減する\n\n    -   作業ごとに2秒のインターバルを設けるということ\n\nさすがに面倒すぎますね。最初は困ると思いますが、2回目以降大変便利に思えるはずです。\n\nひとまずこれでページ上のファイルをダウンロードすることができました！\n\n## 全ページまとめてダウンロード\n\nここまでは1ページ内のファイルをダウンロードする方法について順を追って見てきました。\n\nこのセクションではシェープファイルが1～9ページにまたがって並んでいることを踏まえ、さらに`for`ループを構築し、これまでの作業を9ページにわたって実行できるようにします。\n\n``` r\nfor (i in 1:9) {\n  url <- paste0(\"https://www.e-stat.go.jp/gis/statmap-search?page=\", i, \"&type=2&aggregateUnitForBoundary=S&coordsys=1&format=shape\")\n  \n  html <- read_html_live(url)\n  Sys.sleep(1)\n  \n  links <- html |> \n    html_elements(\"a\") |> \n    html_attr(\"href\")\n  \n  shp_links <- links[grepl(\"data\\\\?dlserveyId=S&\", links)]\n  \n  full_urls <- paste0(base_url, shp_links)\n  \n  for (j in seq_along(full_urls)) {\n    code <- sub(\".*code=([0-9]+).*\", \"\\\\1\", shp_links[j])\n    file_path <- file.path(save_dir, paste0(\"shpfile_\", code, \".zip\"))\n    \n    if (!file.exists(file_path)) {\n      tryCatch({\n        download.file(full_urls[j], destfile = file_path, mode = \"wb\")\n      }, error = function(e) {})\n      Sys.sleep(2)\n    }\n  }\n}\n```\n\n**ポイント：**\n\n1.  URLの`page=`の部分に1から9が入るように設定\n    -   URLを分割し、ページ数の部分に`i`で数字を入れています。\n2.  HTMLを読み込んだ後に`Sys.sleep(1)`で1秒待機\n    -   HTMLを読み込んですぐ次のコマンドに移ると、中身を最後まで読み込まないまま次を実行してしまうので、しっかり読み込めるようにするための設定です。\n3.  `for`ループ2段階目\n    -   大外の`for`で`i`を使っているので、今度は`j`にしています。\n    -   `if`でファイルがない場合に実行するようにしている。\n        -   何らかの理由で途中エラーが発生し、再度全体を実行しても、既にダウンロードできているファイルを重複してダウンロードせずに済みます。\n    -   `tryCatch`は、途中でエラーが起きたときでもスクリプト全体を止めずに柔軟に処理を続けるための関数で、エラーが起きても一旦スキップして最後まで実行します。\n\n見慣れない関数も登場しましたが、このコードを実行すれば9ページ分まとめてダウンロードすることが可能です。\n\n## コードまとめ\n\n最後に今回のコードをまとめます。\n\n``` r\nlibrary(rvest)\nlibrary(here)\n\nfor (i in 1:9) {\n  url <- paste0(\"https://www.e-stat.go.jp/gis/statmap-search?page=\", i, \"&type=2&aggregateUnitForBoundary=S&coordsys=1&format=shape\")\n  \n  html <- read_html_live(url)\n  Sys.sleep(1)\n  \n  links <- html |> \n    html_elements(\"a\") |> \n    html_attr(\"href\")\n  \n  shp_links <- links[grepl(\"data\\\\?dlserveyId=S&\", links)]\n  \n  full_urls <- paste0(base_url, shp_links)\n  \n  for (j in seq_along(full_urls)) {\n    code <- sub(\".*code=([0-9]+).*\", \"\\\\1\", shp_links[j])\n    file_path <- here(\"shpfiles\", paste0(\"shpfile_\", code, \".zip\"))\n    \n    if (!file.exists(file_path)) {\n      tryCatch({\n        download.file(full_urls[j], destfile = file_path, mode = \"wb\")\n      }, error = function(e) {})\n      Sys.sleep(2)\n    }\n  }\n}\n```\n\n## おわりに\n\n今回はRでe-Statのシェープファイルを取得してきました。個人的にもずっとやりたいことだったので、不完全とはいえ、念願かなった感じです。\n\n今後も調査して全ページを一つのループでダウンロードできるようなコードを書いていきたいと思います。\n\n## おまけ\n\nZipファイルでダウンロードすることになりますが、エクスプローラーから直接展開したりファイルを削除するのは面倒だと思います。\n\n解凍し、使用済みのZipファイルを削除するには以下を実行します。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nzip_files <- list.files(save_dir, pattern = \"\\\\.zip$\", full.names = TRUE)\n\nfor (zip_path in zip_files) {\n  unzip(zip_path, exdir = save_dir)\n  file.remove(zip_path)\n}\n```\n:::\n\n\n`unzip()`でZipファイルを解凍して、`file.remove()`でZipファイルを削除します。\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}